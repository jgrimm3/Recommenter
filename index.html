---
layout: default
---

<body>
<h1> Recommenter</h1>
<h3> Authored By: Jared Grimm and Jyan ZÃ¡rate</h3> <h4>CS 525 Information Retrieval, WPI Spring 2020, Professor Kyumin Lee</h4>
<p> A Youtube Recommendation platform based on comments and transcripts to improve performance for recommending to niche communities. This project is strictly for educational research only.</p>

<h4>Documentation</h4>
<ul>
	<li> <a href = https://drive.google.com/open?id=1nLge0NWjqMuf2BtWs-P9dHZXh6xlYqf5xzwpiSNWoso>Project Proposal </a></li>
	<li> <a href = https://drive.google.com/open?id=1iWc9jgxQbUZo-QDcY16dVtEyApXZOWfWqghbdWjLXv0> Proposal Presentation </a></li>
	<li> <a href = https://github.com/jgrimm3/Recommenter> Git Repository</a></li>
	<li> <a href = https://drive.google.com/open?id=1LfZWEn3WGlUfUuI6as9qKxZeETNNzpA8> Video Comment & Transcript Database</a></li>
	<li> <a href = https://drive.google.com/open?id=13Pus9wLQ0FtXDz6jCRb8usKRjOV6DUZa>Jaccard Similarity JSON"</a></li>
	<li> <a href = https://drive.google.com/drive/folders/1eeonxwA0AvI_2JBx4GwR25xeCTkUemQe?usp=sharing>NDCG Evaluation Rubrics</a></li>
	<li> <a href = https://drive.google.com/drive/folders/1GxRAgPyrJFjWha1u1MVUfadw4ArEZzSe?usp=sharing>NDCG Evaluation Results</a></li>

</ul>

<h4>Research Questions</h4>
<p>Can we recommend videos solely off of YouTube comments and transcripts through jaccard similarity?</p>
<p>What are the benefits of solely using comments and transcripts?</p>
<p>How will these recommendations compare with YouTube?</p>
<p>How does this system scale?</p>

<h4>Research Hypothesis</h4>
<p>People of similar communities watch similar content and react with similar comments</p>
<p>Videos that discuss similar things are highly related to each other</p>

<h4> Research Conclusions</h4>
<p>We successfully can recommend videos solely based off of comments and transcripts</p>
<p>Being able to recommend videos only based off of comments and transcripts avoids any biases toward the user demographics, location, and items they may not be interested in compared to other users. It also avoids content creators from being able to bias their material to be recommended more by adding inaccurate tags. Instead every YouTube channel gains a fair chance of being recommended only based on their videos content and how people react to it.</p>
<p>Our recommendations compared to user generated ground truth had an NDCG score of UPDATE while YouTube scored UPDATE</p>
<p>The system was much harder to scale than we anticipated. Gathering video metadata is plausible, however our limited computer resources and time were not enough to generate the scale we anticipated. However once video metadata is extracted and hashing calculations are performed, the system performs well during run time of fetching related videos, relying on query speed of the MongoDB</p>

<h4> Accomplishments</h4>
<ul>
	<li>Developed web crawler to scrape YouTube Comments and Transcripts</li>
	<li>Made a database of roughly 3000 videos Comments, Transcripts, upload date channel Id and Video Ids available for future work</li>
	<li>Indexed over 180,000 scored words based on TF-IDF on transcripts and comments</li>
	<li>Computed minhash with a shingle size of 1 for all 3000 videos comments and transcripts and added the minhash values to Locality Sensitive Hashing Clusters to boost efficiency in comparing hashes for jaccard similarity. </li>
	<li>Computed Jaccard Similarities on all 3000 videos</li>
	<li>Indexed the calculations mentioned above into a MongoDB, so we can query for video and get a JSON output of their Jaccard similarities of Comments and Transcripts </li>
	<li>Weighted comments and transcript similarity the same</li>
	<li>Implemented an automated user test ranking rubric through excel and automatic NDCG calculations comparing our results and YouTubes results to user generated ground truth.</li>
	<li>Generated 12 Rubrics to evaluate NDCG and had 4 outside testers help evaluate them</li>
</ul>

<h4>Challenges</h4>
<ul>
	<li>Gathering data is difficult
		<ul>
			<li>Not all videos had comments allowed or transcripts. Oftentimes movie trailers or music videos disabled subtitles on their videos. This would mean no trailers or music videos can be successfully recommended in our current system.</li>
			<li>Github only allows tracking for files up to 100 mbs, so having to un-track our database and manually share and update over Google Drive was an inconvenience.</li>
			<li>Processing speed for comments, it took roughly 1 minute per 1000 comments on videos to download, with some videos having well over 50,0000 comments, drastically slowing down data mining efforts.</li>
			<li>Indexing unique words to build a dictionary for idf weighting took an extremely long time, so having to reindex if changes were made wasted a lot of time.</li>
			<li>Spelling mistakes in the comments or transcripts caused issues as they were seen as unique words and rated very high by idf ratings, yet were meaningless nonsense. TF weighting and restricting our dictionary to include only words that occurred at least 5 times helped.</li>
			<li>Our LSH hashes did not work as planned as videos did not end up in buckets with similar videos, and also prevented actually relevant videos from being compared. For our current results we excluded LSH buckets before jaccard similarities</li>
			<li>Transcripts that had no meaning such as [Music] or [Applause] caused videos that weren't related at all to be highly related based on transcript</li>
		</ul>
	</li>
	<li>YouTubes API has flaws for this use-case
		<ul>
			<li>Overall YouTubes API was very limited in meeting our needs for data at scale. We had to construct our own database using a combination of API calls, other libraries, and custom crawlers.</li>
			<li>YouTube quota limit extremely low. With its 10,000 credit per day limit were only able to process roughly 100 videos a day using YouTubes API alone</li>
			<li>YouTube doesnt specify if related videos returned by "relatedToVideoId" which "retrieves a list of videos that are related to the video" are ordered by relevance at all. We assumed they are ranked from most relevant to least relevant in order to calculate NDCG</li>
		</ul>
	</li>
</ul>


<h4>Future Work</h4>
<p>While we successfully demonstrated the feasibility of using video comments and transcripts to target niche communities, we suggest the following improvements or additions to our work</p>

<ul>
	<li><b>Optimize</b>  Leverage multi-threading and cloud computing resources to offload the data scraping, term scoring, and hashing functions to be able to gather more data much faster</li>
	<li><b>Graphic Implementation</b> Utilize MonkeyTamper Google chrome extension scripts to replace YouTubes Recommended videos with our Recommenter generated links. </li>
	<li><b>Learning to Rank</b> Implement pairwise learning to rank in order to better weight the scores of comment similarity vs transcript similarity, or any other added features. We recommend attempting to score words based on Okapi-BM 25 instead of Tf-IDF to factor in the varying length of transcripts and comments. To do this you must store all original comments and transcripts, and use a python package available through pip install rank-bm25 </li>
	<li><b>Request Increased YouTube API Quota</b> Given more time, one can submit a request to YouTube through Google Cloud services to raise the API quota for their project, this would allow for faster data collection </li>

</ul>

<h4> Dependencies </h4>
<p>Please download or clone our github repository and run <i>pip3 install -r requirements.txt</i></p>

<h4> Running Instructions</h4>
<ul>
	<li><b> To gather YouTube video comments and transcripts</b>
		<ul>
			<li>Everything needed to gather new comments and transcripts are stored in the <i> Video Database Generation </i> Folder. Make that folder your working directory with all dependencies installed.</li>
			<li>You will need a google API key For YouTube API v3 in order to run, please put the key on line 30 of <i>DataScraper.py</i></li>
			<li>Download the "Video and Comment Database" found on this web-page and insure videoInfo.db is in the working directory locally.</li>
			<li> If you'd rather create an empty database and start fresh, the functions to create a new database and table layouts are available in SQL.py to run from command line. Please read the file for more info</li>
			<li>Run <i> DataScraper.py</i></li>
			<li><b> Be sure to change the seed video after each run</b></li>
			<li>You can change the number of videos to scrape for and the category of videos by changing the <i>Seed</i> and <i>Count</i> variables located on the first two lines of code</li>
			<li> To change the threshold of comments that are gathered by Utuby (all comments will be retrieved) vs YouTube API (only 100-500 comments will be retrieved)  on line 15 of <i> fetch_comments.py.</i> higher means longer scraping, lower means more chance of rate limit. </li>
		</ul>
	</li>


	<li><b>To evaluate NDCG</b>
		<ul>
			<li>You will once again need to place a YouTube API v3 key on line 14 of <i>createRubric.py</i></li>
			<li> Run <i>createRubric.py</i> with (url to the starting Youtube video) . </li>
			<li> This will generate an excel file populated with 10 links from YouTube recommendations and 10 links from Recommenter recommendations.</li>
			<li> A user will numerically rank in column b from 1-5 their interpretation of how similar the videos are to the input video.</li>
			<li> Once all videos are ranked and the file is saved locally, run <i>ndcg.py</i>. </li>
			<li> The Ground truth will be the top 10 from the union of both sets</li>
			<li> The Ground truth video Ids will be shown, along side the NDCG Scores for Youtube and Recommenter recommendations.</li>
		</ul>
	</li>
</ul>
</body>

